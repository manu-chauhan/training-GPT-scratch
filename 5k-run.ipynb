{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import tiktoken\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size:int = 1024 # this is max sequence len\n",
    "    vocab_size:int = 50304 #50257 # total vocab including 256 bytes + 1 special token (<|endoftext|>) and 1000-257 BPE merges\n",
    "    n_layer:int = 12 # number of layers \n",
    "    n_head:int = 12 # total number of attention heads\n",
    "    n_embd: int = 768 # embedding dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        n_head = config.n_head\n",
    "        n_embd = config.n_embd\n",
    "        \n",
    "        assert n_embd % n_head == 0\n",
    "        \n",
    "        # query, key, value prjections all combined\n",
    "        self.c_attn = nn.Linear(n_embd, 3 * n_embd)\n",
    "        \n",
    "        # output projection, after `v` is already multiplied with attention_scores\n",
    "        self.c_proj = nn.Linear(n_embd, n_embd)\n",
    "        \n",
    "        self.c_proj.NANOGPT_SCALE_INIT=1\n",
    "\n",
    "        block_size = config.block_size\n",
    "        \n",
    "        self.register_buffer('bias', torch.tril(torch.ones(block_size, block_size)).view(1, 1, block_size, block_size))\n",
    "        \n",
    "        self.n_embd = n_embd\n",
    "        self.n_head = n_head\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch_size, sequence_len, embedding_dim (n_embd)\n",
    "        # total dim = n_head * head_size\n",
    "        # example GPT2 has 12 heads with each hs = 64 thus C= 12*64 = 768\n",
    "\n",
    "        qkv = self.c_attn(x) # get combined qkv matix B, T, n_embd * 3(768*3=2304)\n",
    "\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2) # each item gets n_embd size, dimension against two \n",
    "\n",
    "        # b, seq, n_embd -> b, seq, n_heads, head_size -> b, n_heads, seq_len, head_size\n",
    "        q = q.view(B, T, self.n_head, C//self.n_head).transpose(1, 2)\n",
    "        # final-> bs, n_heads, seq_len, mini-n_head_embd\n",
    "\n",
    "        k = k.view(B, T, self.n_head, C//self.n_head).transpose(1, 2)\n",
    "        \n",
    "        v = v.view(B, T, self.n_head, C//self.n_head).transpose(1, 2)\n",
    "        \n",
    "        # # print(f\"shape of q: {q.shape}... shape of k : {k.shape}\")\n",
    "        \n",
    "        # attn = (q @ k.transpose(-2, -1))/(math.sqrt(k.shape[-1]))\n",
    "\n",
    "        # # apply masked fill at places where mask ==0, remember tril is lower triangle\n",
    "        # attn = attn.masked_fill(mask = self.bias[ : , : , :T, :T] == 0, value=float('-inf'))\n",
    "        \n",
    "        # attn = F.softmax(attn, dim=-1)\n",
    "        \n",
    "        # y = attn @ v # B, n_heads, T/seq, T @ B, n_heads, T/Seq, head_size) -> B, n_heads, T, head_size\n",
    "\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True) # flash attention\n",
    "        \n",
    "        # transpose y to merge all n_heads. B, n_heads, T, head_size -> transpose B, T, n_heads, head_size -> view B, T, Channel_size/n_emb 768 \n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        \n",
    "        # out projection, B, T, C -> B, T, C\n",
    "        y = self.c_proj(y)\n",
    "        \n",
    "        return y\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu = nn.GELU(approximate='tanh')\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "    \n",
    "        self.transformer = nn.ModuleDict(\n",
    "            dict(\n",
    "                wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "                wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "                h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "                ln_f = nn.LayerNorm(config.n_embd)\n",
    "            ))\n",
    "\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        \n",
    "        # weight sharing\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "        # weight initialization\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02\n",
    "            if hasattr(module, 'NANOGPT_SCALE_INIT'):\n",
    "                std *= (2 * self.config.n_layer) ** -0.5\n",
    "\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            \n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.normal_(module.bias, mean=0.0, std=std)\n",
    "        \n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.size() # batch , seq_len\n",
    "\n",
    "        # check if incoming seq_len of idx is within limits\n",
    "        assert T <= self.config.block_size, f\"Cannot proceed as your Sequence len : {T} is more than {self.config.block_size}\"\n",
    "\n",
    "        # forward for token and position encodings\n",
    "        # shape (T)\n",
    "        pos = torch.arange(0, T, dtype=torch.int32, device=idx.device)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embds of shape (T, n_embd)\n",
    "        token_emb = self.transformer.wte(idx) # token embds of shape (Batch, T/seq_len, n_embd)\n",
    "\n",
    "        x = pos_emb + token_emb\n",
    "\n",
    "        # now forward through transformer blocks\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        \n",
    "        # pass through final layernorm\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        # pass through final LM_HEAD\n",
    "        logits = self.lm_head(x) # shape (Batch_size, T, vocab_size)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_name):\n",
    "        \"\"\"for loading pre-trained GPT model weights from HuggingFace\"\"\"\n",
    "        assert model_name in ['gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl']\n",
    "\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        \n",
    "        print(f\"Loading weights from pretrained GPT model: {model_name}\")\n",
    "\n",
    "        # n_layer, n_head, n_embd for each model_name\n",
    "        config_args = {\n",
    "            'gpt2':        dict(n_layer=12, n_embd=768, n_head=12), # has 124M params\n",
    "            'gpt2-medium': dict(n_layer=24, n_embd=1024, n_head=16), # 350M params\n",
    "            'gpt2-large':  dict(n_layer=36, n_embd=1280, n_head=20), # 774M params\n",
    "            'gpt2-xl':     dict(n_layer=48, n_embd=1600, n_head=25) # 1558M params\n",
    "            \n",
    "        }[model_name]\n",
    "\n",
    "        config_args['vocab_size'] = 50257 # same for all GPT2 checkpoints\n",
    "        config_args['block_size']= 1024 # max seq len 1024 for all GPT2 checkpoints\n",
    "\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [key for key in sd_keys if not key.endswith('.attn.bias')] # discard this mask, not a parameter\n",
    "\n",
    "        # initialize transformer model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "        sd_hf_keys = [key for key in sd_hf.keys() if not key.endswith('.attn.masked_bias')] # to discard, not a parameter\n",
    "        sd_hf_keys = [key for key in sd_hf.keys() if not key.endswith('.attn.bias')] # to discard, not a param\n",
    "\n",
    "        # transposing these to match openai's Conv1d usage with Linear layer\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "\n",
    "        print(\"=======\\nDifference in keys: \", set(sd_keys)- set(sd_hf_keys))\n",
    "        assert len(sd_keys) == len(sd_hf_keys), f\"mismatched keys: sd_keys {len(sd_keys)} != sd_hd_keys {len(sd_hf_keys)}\"\n",
    "\n",
    "        for key in sd_hf_keys:\n",
    "            if any(key.endswith(w) for w in transposed):\n",
    "                assert sd_hf[key].shape[::-1] == sd[key].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[key].copy_(sd_hf[key].t())\n",
    "            else:\n",
    "                # simple copy for other params\n",
    "                assert sd_hf[key].shape == sd[key].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[key].copy_(sd_hf[key])\n",
    "                    \n",
    "        return model            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T):\n",
    "        self.B = B \n",
    "        self.T = T \n",
    "\n",
    "        with open('input.txt', 'r') as f:\n",
    "            text = f.read()\n",
    "        enc = tiktoken.get_encoding('gpt2')\n",
    "        tokens = enc.encode(text)\n",
    "        self.tokens = torch.tensor(tokens)\n",
    "        print(f'loaded len : {len(self.tokens)}')\n",
    "        print(f'1 epoch = {len(self.tokens)//(B*T)} batches ')\n",
    "        self.current_position = 0\n",
    "    \n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[self.current_position: self.current_position + (B*T) + 1]\n",
    "        x = buf[:-1].view(B, T)\n",
    "        y = buf[1:].view(B, T)\n",
    "\n",
    "        self.current_position += (B*T)\n",
    "        \n",
    "        if self.current_position + (B*T+1) > len(self.tokens):\n",
    "            self.current_position = 0\n",
    "        return x, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "highest\n"
     ]
    }
   ],
   "source": [
    "current_precision = torch.get_float32_matmul_precision()\n",
    "print(current_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lr = 6e-4\n",
    "min_lr = 0.1 * max_lr\n",
    "warmup_steps = 10\n",
    "max_steps = 5001\n",
    "\n",
    "def get_lr(iteration):\n",
    "    if iteration < warmup_steps:\n",
    "        return max_lr * (iteration + 1) / warmup_steps\n",
    "    if iteration > max_steps:\n",
    "        return min_lr\n",
    "    \n",
    "    decay_ratio = (iteration - warmup_steps) / (max_steps - warmup_steps)\n",
    "\n",
    "    assert 0<= decay_ratio <= 1\n",
    "\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "    return min_lr + coeff * (max_lr - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "torch.set_default_device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT(GPTConfig()).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, betas=(0.9, 0.95), eps=1e-8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded len : 338025\n",
      "1 epoch = 55 batches \n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoaderLite(B=6, T=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): GPT(\n",
       "    (transformer): ModuleDict(\n",
       "      (wte): Embedding(50304, 768)\n",
       "      (wpe): Embedding(1024, 768)\n",
       "      (h): ModuleList(\n",
       "        (0-11): 12 x Block(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): CausalSelfAttention(\n",
       "            (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): GELU(approximate='tanh')\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=50304, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124475904"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters() if p.requires_grad) # 163109376 # 124475904"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "torch.set_default_device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6050/3327562475.py:14: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  norm = torch.nn.utils.clip_grad_norm(model.parameters(), 1.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step : 1 | loss: 10.967509269714355 | dt: 22181.45 ms | tokens/sec: 0.28 | norm: 27.59\n",
      "step : 2 | loss: 9.587400436401367 | dt: 160.60 ms | tokens/sec: 38.26 | norm: 9.12\n",
      "step : 3 | loss: 9.105223655700684 | dt: 162.28 ms | tokens/sec: 37.86 | norm: 3.80\n",
      "step : 4 | loss: 9.088119506835938 | dt: 159.25 ms | tokens/sec: 38.58 | norm: 7.55\n",
      "step : 5 | loss: 8.63983154296875 | dt: 160.60 ms | tokens/sec: 38.26 | norm: 2.78\n",
      "step : 6 | loss: 8.500044822692871 | dt: 159.48 ms | tokens/sec: 38.52 | norm: 3.86\n",
      "step : 7 | loss: 8.25190258026123 | dt: 159.88 ms | tokens/sec: 38.43 | norm: 2.66\n",
      "step : 8 | loss: 8.106292724609375 | dt: 158.65 ms | tokens/sec: 38.73 | norm: 2.70\n",
      "step : 9 | loss: 7.8846235275268555 | dt: 159.03 ms | tokens/sec: 38.63 | norm: 1.78\n",
      "step : 10 | loss: 7.458106517791748 | dt: 159.05 ms | tokens/sec: 38.63 | norm: 2.76\n",
      "step : 11 | loss: 7.141818046569824 | dt: 158.42 ms | tokens/sec: 38.78 | norm: 1.80\n",
      "step : 12 | loss: 6.820505142211914 | dt: 160.68 ms | tokens/sec: 38.24 | norm: 3.02\n",
      "step : 13 | loss: 6.726710319519043 | dt: 159.71 ms | tokens/sec: 38.47 | norm: 1.62\n",
      "step : 14 | loss: 6.9565887451171875 | dt: 159.95 ms | tokens/sec: 38.41 | norm: 4.33\n",
      "step : 15 | loss: 6.82418966293335 | dt: 158.55 ms | tokens/sec: 38.75 | norm: 3.41\n",
      "step : 16 | loss: 6.719030380249023 | dt: 159.79 ms | tokens/sec: 38.45 | norm: 1.37\n",
      "step : 17 | loss: 6.797262191772461 | dt: 159.57 ms | tokens/sec: 38.50 | norm: 1.68\n",
      "step : 18 | loss: 6.663741588592529 | dt: 158.71 ms | tokens/sec: 38.71 | norm: 1.14\n",
      "step : 19 | loss: 6.730315208435059 | dt: 159.94 ms | tokens/sec: 38.41 | norm: 1.65\n",
      "step : 20 | loss: 6.5407304763793945 | dt: 159.32 ms | tokens/sec: 38.56 | norm: 1.76\n",
      "step : 21 | loss: 6.410836696624756 | dt: 159.57 ms | tokens/sec: 38.50 | norm: 1.65\n",
      "step : 22 | loss: 6.44638204574585 | dt: 161.66 ms | tokens/sec: 38.01 | norm: 1.18\n",
      "step : 23 | loss: 6.636126518249512 | dt: 161.06 ms | tokens/sec: 38.15 | norm: 1.08\n",
      "step : 24 | loss: 6.514188289642334 | dt: 161.77 ms | tokens/sec: 37.98 | norm: 1.73\n",
      "step : 25 | loss: 6.440850734710693 | dt: 161.17 ms | tokens/sec: 38.12 | norm: 1.16\n",
      "step : 26 | loss: 6.407115936279297 | dt: 161.67 ms | tokens/sec: 38.00 | norm: 1.14\n",
      "step : 27 | loss: 6.194819450378418 | dt: 161.71 ms | tokens/sec: 37.99 | norm: 1.24\n",
      "step : 28 | loss: 6.312343120574951 | dt: 161.67 ms | tokens/sec: 38.00 | norm: 0.82\n",
      "step : 29 | loss: 6.135289669036865 | dt: 160.59 ms | tokens/sec: 38.26 | norm: 1.14\n",
      "step : 30 | loss: 6.32150411605835 | dt: 162.32 ms | tokens/sec: 37.85 | norm: 1.21\n",
      "step : 31 | loss: 6.241056442260742 | dt: 161.70 ms | tokens/sec: 38.00 | norm: 0.98\n",
      "step : 32 | loss: 6.055328369140625 | dt: 162.14 ms | tokens/sec: 37.89 | norm: 1.01\n",
      "step : 33 | loss: 6.0929107666015625 | dt: 158.69 ms | tokens/sec: 38.72 | norm: 0.85\n",
      "step : 34 | loss: 6.110854148864746 | dt: 159.40 ms | tokens/sec: 38.54 | norm: 0.80\n",
      "step : 35 | loss: 6.2206010818481445 | dt: 158.73 ms | tokens/sec: 38.71 | norm: 0.89\n",
      "step : 36 | loss: 6.490775108337402 | dt: 159.69 ms | tokens/sec: 38.47 | norm: 1.26\n",
      "step : 37 | loss: 6.307951927185059 | dt: 159.01 ms | tokens/sec: 38.64 | norm: 1.24\n",
      "step : 38 | loss: 6.561453819274902 | dt: 160.13 ms | tokens/sec: 38.37 | norm: 0.90\n",
      "step : 39 | loss: 6.456516265869141 | dt: 158.46 ms | tokens/sec: 38.77 | norm: 0.98\n",
      "step : 40 | loss: 6.358968734741211 | dt: 158.73 ms | tokens/sec: 38.71 | norm: 1.02\n",
      "step : 41 | loss: 6.254167556762695 | dt: 158.34 ms | tokens/sec: 38.80 | norm: 0.86\n",
      "step : 42 | loss: 6.306066513061523 | dt: 162.22 ms | tokens/sec: 37.87 | norm: 0.85\n",
      "step : 43 | loss: 6.1236066818237305 | dt: 161.55 ms | tokens/sec: 38.03 | norm: 1.22\n",
      "step : 44 | loss: 6.277118682861328 | dt: 160.87 ms | tokens/sec: 38.19 | norm: 1.13\n",
      "step : 45 | loss: 6.487403869628906 | dt: 159.80 ms | tokens/sec: 38.45 | norm: 1.22\n",
      "step : 46 | loss: 6.290515899658203 | dt: 161.51 ms | tokens/sec: 38.04 | norm: 1.13\n",
      "step : 47 | loss: 6.150565147399902 | dt: 159.96 ms | tokens/sec: 38.41 | norm: 1.07\n",
      "step : 48 | loss: 6.220577239990234 | dt: 159.17 ms | tokens/sec: 38.60 | norm: 0.84\n",
      "step : 49 | loss: 6.278977394104004 | dt: 160.65 ms | tokens/sec: 38.24 | norm: 1.20\n",
      "step : 50 | loss: 6.158618927001953 | dt: 160.95 ms | tokens/sec: 38.17 | norm: 1.04\n",
      "step : 51 | loss: 6.2821879386901855 | dt: 161.31 ms | tokens/sec: 38.09 | norm: 0.88\n",
      "step : 52 | loss: 5.945176601409912 | dt: 161.04 ms | tokens/sec: 38.15 | norm: 0.87\n",
      "step : 53 | loss: 5.915252685546875 | dt: 158.77 ms | tokens/sec: 38.70 | norm: 1.03\n",
      "step : 54 | loss: 6.323966026306152 | dt: 159.48 ms | tokens/sec: 38.53 | norm: 1.42\n",
      "step : 55 | loss: 6.2671332359313965 | dt: 159.30 ms | tokens/sec: 38.57 | norm: 1.27\n",
      "step : 56 | loss: 6.300098419189453 | dt: 159.66 ms | tokens/sec: 38.48 | norm: 1.30\n",
      "step : 57 | loss: 6.266829967498779 | dt: 158.25 ms | tokens/sec: 38.82 | norm: 1.18\n",
      "step : 58 | loss: 6.166554927825928 | dt: 159.77 ms | tokens/sec: 38.46 | norm: 1.13\n",
      "step : 59 | loss: 5.938294410705566 | dt: 157.68 ms | tokens/sec: 38.96 | norm: 1.27\n",
      "step : 60 | loss: 5.8383708000183105 | dt: 158.38 ms | tokens/sec: 38.79 | norm: 0.98\n",
      "step : 61 | loss: 5.823614120483398 | dt: 159.86 ms | tokens/sec: 38.43 | norm: 1.41\n",
      "step : 62 | loss: 5.815214157104492 | dt: 157.70 ms | tokens/sec: 38.96 | norm: 1.20\n",
      "step : 63 | loss: 5.953930854797363 | dt: 157.66 ms | tokens/sec: 38.97 | norm: 0.93\n",
      "step : 64 | loss: 6.078394889831543 | dt: 158.32 ms | tokens/sec: 38.81 | norm: 1.61\n",
      "step : 65 | loss: 5.97088623046875 | dt: 157.69 ms | tokens/sec: 38.96 | norm: 1.51\n",
      "step : 66 | loss: 5.866326808929443 | dt: 158.30 ms | tokens/sec: 38.81 | norm: 1.30\n",
      "step : 67 | loss: 5.727490425109863 | dt: 157.71 ms | tokens/sec: 38.96 | norm: 1.14\n",
      "step : 68 | loss: 5.904729843139648 | dt: 159.19 ms | tokens/sec: 38.59 | norm: 1.11\n",
      "step : 69 | loss: 6.0605573654174805 | dt: 157.77 ms | tokens/sec: 38.94 | norm: 1.09\n",
      "step : 70 | loss: 5.984042167663574 | dt: 158.35 ms | tokens/sec: 38.80 | norm: 1.14\n",
      "step : 71 | loss: 6.06726598739624 | dt: 160.44 ms | tokens/sec: 38.30 | norm: 1.21\n",
      "step : 72 | loss: 6.119211196899414 | dt: 159.59 ms | tokens/sec: 38.50 | norm: 0.99\n",
      "step : 73 | loss: 5.993075370788574 | dt: 160.84 ms | tokens/sec: 38.20 | norm: 1.04\n",
      "step : 74 | loss: 6.05143928527832 | dt: 159.48 ms | tokens/sec: 38.53 | norm: 1.48\n",
      "step : 75 | loss: 5.944252014160156 | dt: 159.83 ms | tokens/sec: 38.44 | norm: 0.99\n",
      "step : 76 | loss: 5.777759075164795 | dt: 161.07 ms | tokens/sec: 38.15 | norm: 0.99\n",
      "step : 77 | loss: 5.8930864334106445 | dt: 160.15 ms | tokens/sec: 38.37 | norm: 1.18\n",
      "step : 78 | loss: 6.057164669036865 | dt: 161.02 ms | tokens/sec: 38.16 | norm: 1.17\n",
      "step : 79 | loss: 5.954768180847168 | dt: 160.34 ms | tokens/sec: 38.32 | norm: 1.02\n",
      "step : 80 | loss: 5.854065895080566 | dt: 159.78 ms | tokens/sec: 38.45 | norm: 1.01\n",
      "step : 81 | loss: 5.849109649658203 | dt: 163.06 ms | tokens/sec: 37.68 | norm: 1.19\n",
      "step : 82 | loss: 5.646325588226318 | dt: 161.15 ms | tokens/sec: 38.13 | norm: 1.10\n",
      "step : 83 | loss: 5.775731086730957 | dt: 160.24 ms | tokens/sec: 38.34 | norm: 0.89\n",
      "step : 84 | loss: 5.636594772338867 | dt: 161.46 ms | tokens/sec: 38.05 | norm: 1.02\n",
      "step : 85 | loss: 5.873015403747559 | dt: 160.12 ms | tokens/sec: 38.37 | norm: 0.98\n",
      "step : 86 | loss: 5.812243461608887 | dt: 159.71 ms | tokens/sec: 38.47 | norm: 1.09\n",
      "step : 87 | loss: 5.592902660369873 | dt: 161.43 ms | tokens/sec: 38.06 | norm: 1.05\n",
      "step : 88 | loss: 5.6357879638671875 | dt: 159.58 ms | tokens/sec: 38.50 | norm: 0.93\n",
      "step : 89 | loss: 5.654515743255615 | dt: 160.32 ms | tokens/sec: 38.32 | norm: 0.86\n",
      "step : 90 | loss: 5.791955947875977 | dt: 162.05 ms | tokens/sec: 37.91 | norm: 0.98\n",
      "step : 91 | loss: 5.994649410247803 | dt: 162.37 ms | tokens/sec: 37.84 | norm: 1.20\n",
      "step : 92 | loss: 5.813836097717285 | dt: 160.38 ms | tokens/sec: 38.31 | norm: 1.28\n",
      "step : 93 | loss: 6.089749813079834 | dt: 161.77 ms | tokens/sec: 37.98 | norm: 1.15\n",
      "step : 94 | loss: 5.9617719650268555 | dt: 160.69 ms | tokens/sec: 38.24 | norm: 1.14\n",
      "step : 95 | loss: 5.815345764160156 | dt: 159.94 ms | tokens/sec: 38.42 | norm: 1.05\n",
      "step : 96 | loss: 5.781889915466309 | dt: 161.87 ms | tokens/sec: 37.96 | norm: 1.03\n",
      "step : 97 | loss: 5.719932556152344 | dt: 160.48 ms | tokens/sec: 38.29 | norm: 1.13\n",
      "step : 98 | loss: 5.519134521484375 | dt: 159.85 ms | tokens/sec: 38.44 | norm: 1.17\n",
      "step : 99 | loss: 5.667886257171631 | dt: 161.85 ms | tokens/sec: 37.96 | norm: 1.02\n",
      "step : 100 | loss: 5.854735374450684 | dt: 162.88 ms | tokens/sec: 37.72 | norm: 1.46\n",
      "step : 101 | loss: 5.6818671226501465 | dt: 160.52 ms | tokens/sec: 38.28 | norm: 1.10\n",
      "step : 102 | loss: 5.4687652587890625 | dt: 161.30 ms | tokens/sec: 38.09 | norm: 1.22\n",
      "step : 103 | loss: 5.7213287353515625 | dt: 160.48 ms | tokens/sec: 38.28 | norm: 1.03\n",
      "step : 104 | loss: 5.588007926940918 | dt: 160.49 ms | tokens/sec: 38.28 | norm: 1.01\n",
      "step : 105 | loss: 5.479866027832031 | dt: 162.06 ms | tokens/sec: 37.91 | norm: 1.04\n",
      "step : 106 | loss: 5.600752830505371 | dt: 159.64 ms | tokens/sec: 38.49 | norm: 0.86\n",
      "step : 107 | loss: 5.287364959716797 | dt: 156.70 ms | tokens/sec: 39.21 | norm: 0.93\n",
      "step : 108 | loss: 5.114194869995117 | dt: 156.65 ms | tokens/sec: 39.22 | norm: 0.97\n",
      "step : 109 | loss: 5.765021800994873 | dt: 155.61 ms | tokens/sec: 39.48 | norm: 1.16\n",
      "step : 110 | loss: 5.687748908996582 | dt: 159.01 ms | tokens/sec: 38.64 | norm: 1.31\n",
      "step : 111 | loss: 5.802241325378418 | dt: 156.15 ms | tokens/sec: 39.35 | norm: 1.16\n",
      "step : 112 | loss: 5.796989440917969 | dt: 157.39 ms | tokens/sec: 39.04 | norm: 1.04\n",
      "step : 113 | loss: 5.727221488952637 | dt: 155.42 ms | tokens/sec: 39.53 | norm: 1.09\n",
      "step : 114 | loss: 5.508151054382324 | dt: 156.56 ms | tokens/sec: 39.24 | norm: 1.38\n",
      "step : 115 | loss: 5.443116664886475 | dt: 156.77 ms | tokens/sec: 39.19 | norm: 1.29\n",
      "step : 116 | loss: 5.440939903259277 | dt: 155.86 ms | tokens/sec: 39.42 | norm: 1.18\n",
      "step : 117 | loss: 5.445209503173828 | dt: 157.45 ms | tokens/sec: 39.02 | norm: 0.85\n",
      "step : 118 | loss: 5.551394462585449 | dt: 155.12 ms | tokens/sec: 39.61 | norm: 0.78\n",
      "step : 119 | loss: 5.658926010131836 | dt: 156.74 ms | tokens/sec: 39.20 | norm: 1.42\n",
      "step : 120 | loss: 5.554033279418945 | dt: 157.85 ms | tokens/sec: 38.92 | norm: 1.08\n",
      "step : 121 | loss: 5.455596923828125 | dt: 158.05 ms | tokens/sec: 38.87 | norm: 0.99\n",
      "step : 122 | loss: 5.25527286529541 | dt: 156.84 ms | tokens/sec: 39.17 | norm: 1.01\n",
      "step : 123 | loss: 5.49061393737793 | dt: 155.71 ms | tokens/sec: 39.46 | norm: 1.09\n",
      "step : 124 | loss: 5.55718994140625 | dt: 157.93 ms | tokens/sec: 38.90 | norm: 0.99\n",
      "step : 125 | loss: 5.483733177185059 | dt: 155.10 ms | tokens/sec: 39.61 | norm: 1.04\n",
      "step : 126 | loss: 5.66754674911499 | dt: 156.42 ms | tokens/sec: 39.28 | norm: 0.94\n",
      "step : 127 | loss: 5.7416791915893555 | dt: 156.10 ms | tokens/sec: 39.36 | norm: 0.99\n",
      "step : 128 | loss: 5.66318416595459 | dt: 155.56 ms | tokens/sec: 39.49 | norm: 1.04\n",
      "step : 129 | loss: 5.713062286376953 | dt: 156.50 ms | tokens/sec: 39.26 | norm: 0.96\n",
      "step : 130 | loss: 5.5810956954956055 | dt: 158.15 ms | tokens/sec: 38.85 | norm: 0.89\n",
      "step : 131 | loss: 5.35999059677124 | dt: 156.52 ms | tokens/sec: 39.25 | norm: 0.93\n",
      "step : 132 | loss: 5.527796745300293 | dt: 155.63 ms | tokens/sec: 39.48 | norm: 0.91\n",
      "step : 133 | loss: 5.732283592224121 | dt: 156.55 ms | tokens/sec: 39.25 | norm: 1.02\n",
      "step : 134 | loss: 5.592780113220215 | dt: 155.79 ms | tokens/sec: 39.44 | norm: 0.95\n",
      "step : 135 | loss: 5.525254726409912 | dt: 156.94 ms | tokens/sec: 39.15 | norm: 1.07\n",
      "step : 136 | loss: 5.544522762298584 | dt: 155.82 ms | tokens/sec: 39.43 | norm: 0.94\n",
      "step : 137 | loss: 5.328550338745117 | dt: 156.26 ms | tokens/sec: 39.32 | norm: 0.95\n",
      "step : 138 | loss: 5.483357906341553 | dt: 155.66 ms | tokens/sec: 39.47 | norm: 1.08\n",
      "step : 139 | loss: 5.302725791931152 | dt: 155.90 ms | tokens/sec: 39.41 | norm: 1.02\n",
      "step : 140 | loss: 5.5668487548828125 | dt: 160.00 ms | tokens/sec: 38.40 | norm: 0.84\n",
      "step : 141 | loss: 5.501237392425537 | dt: 155.88 ms | tokens/sec: 39.42 | norm: 0.86\n",
      "step : 142 | loss: 5.254765033721924 | dt: 157.90 ms | tokens/sec: 38.91 | norm: 1.00\n",
      "step : 143 | loss: 5.293604850769043 | dt: 155.34 ms | tokens/sec: 39.55 | norm: 0.80\n",
      "step : 144 | loss: 5.326774597167969 | dt: 156.57 ms | tokens/sec: 39.24 | norm: 0.80\n",
      "step : 145 | loss: 5.422454833984375 | dt: 156.90 ms | tokens/sec: 39.16 | norm: 0.81\n",
      "step : 146 | loss: 5.723688125610352 | dt: 156.37 ms | tokens/sec: 39.29 | norm: 1.25\n",
      "step : 147 | loss: 5.546222686767578 | dt: 158.54 ms | tokens/sec: 38.75 | norm: 1.19\n",
      "step : 148 | loss: 5.845714092254639 | dt: 156.55 ms | tokens/sec: 39.25 | norm: 1.11\n",
      "step : 149 | loss: 5.7024455070495605 | dt: 158.86 ms | tokens/sec: 38.68 | norm: 1.00\n",
      "step : 150 | loss: 5.536028861999512 | dt: 158.94 ms | tokens/sec: 38.66 | norm: 1.07\n",
      "step : 151 | loss: 5.516026973724365 | dt: 157.86 ms | tokens/sec: 38.92 | norm: 0.85\n",
      "step : 152 | loss: 5.420510768890381 | dt: 157.51 ms | tokens/sec: 39.01 | norm: 0.95\n",
      "step : 153 | loss: 5.226924896240234 | dt: 157.09 ms | tokens/sec: 39.11 | norm: 1.41\n",
      "step : 154 | loss: 5.389876365661621 | dt: 157.57 ms | tokens/sec: 38.99 | norm: 1.20\n",
      "step : 155 | loss: 5.5014190673828125 | dt: 157.03 ms | tokens/sec: 39.13 | norm: 1.08\n",
      "step : 156 | loss: 5.328715801239014 | dt: 157.74 ms | tokens/sec: 38.95 | norm: 1.00\n",
      "step : 157 | loss: 5.1177263259887695 | dt: 157.84 ms | tokens/sec: 38.93 | norm: 1.08\n",
      "step : 158 | loss: 5.450950622558594 | dt: 156.21 ms | tokens/sec: 39.33 | norm: 0.84\n",
      "step : 159 | loss: 5.281645774841309 | dt: 157.02 ms | tokens/sec: 39.13 | norm: 1.08\n",
      "step : 160 | loss: 5.195952892303467 | dt: 159.01 ms | tokens/sec: 38.64 | norm: 1.07\n",
      "step : 161 | loss: 5.31247091293335 | dt: 157.38 ms | tokens/sec: 39.04 | norm: 0.85\n",
      "step : 162 | loss: 5.023594379425049 | dt: 157.55 ms | tokens/sec: 39.00 | norm: 1.00\n",
      "step : 163 | loss: 4.832080841064453 | dt: 157.26 ms | tokens/sec: 39.07 | norm: 1.02\n",
      "step : 164 | loss: 5.523252010345459 | dt: 157.58 ms | tokens/sec: 38.99 | norm: 1.13\n",
      "step : 165 | loss: 5.408926963806152 | dt: 156.56 ms | tokens/sec: 39.24 | norm: 1.15\n",
      "step : 166 | loss: 5.557374477386475 | dt: 158.09 ms | tokens/sec: 38.86 | norm: 1.13\n",
      "step : 167 | loss: 5.56088924407959 | dt: 156.17 ms | tokens/sec: 39.34 | norm: 1.16\n",
      "step : 168 | loss: 5.483892440795898 | dt: 157.55 ms | tokens/sec: 39.00 | norm: 1.09\n",
      "step : 169 | loss: 5.222398281097412 | dt: 156.83 ms | tokens/sec: 39.18 | norm: 1.22\n",
      "step : 170 | loss: 5.17518949508667 | dt: 159.21 ms | tokens/sec: 38.59 | norm: 0.95\n",
      "step : 171 | loss: 5.184739589691162 | dt: 158.62 ms | tokens/sec: 38.73 | norm: 1.10\n",
      "step : 172 | loss: 5.2227373123168945 | dt: 156.44 ms | tokens/sec: 39.27 | norm: 1.01\n",
      "step : 173 | loss: 5.324985027313232 | dt: 158.69 ms | tokens/sec: 38.72 | norm: 0.91\n",
      "step : 174 | loss: 5.362657070159912 | dt: 156.49 ms | tokens/sec: 39.26 | norm: 1.40\n",
      "step : 175 | loss: 5.294258117675781 | dt: 158.42 ms | tokens/sec: 38.78 | norm: 1.13\n",
      "step : 176 | loss: 5.202219009399414 | dt: 155.72 ms | tokens/sec: 39.46 | norm: 1.08\n",
      "step : 177 | loss: 5.0031633377075195 | dt: 157.34 ms | tokens/sec: 39.05 | norm: 0.98\n",
      "step : 178 | loss: 5.242551326751709 | dt: 157.51 ms | tokens/sec: 39.01 | norm: 0.87\n",
      "step : 179 | loss: 5.260554790496826 | dt: 156.34 ms | tokens/sec: 39.30 | norm: 1.02\n",
      "step : 180 | loss: 5.17871618270874 | dt: 160.70 ms | tokens/sec: 38.23 | norm: 1.06\n",
      "step : 181 | loss: 5.393174648284912 | dt: 156.51 ms | tokens/sec: 39.26 | norm: 0.82\n",
      "step : 182 | loss: 5.484038352966309 | dt: 158.09 ms | tokens/sec: 38.86 | norm: 0.79\n",
      "step : 183 | loss: 5.391676902770996 | dt: 156.56 ms | tokens/sec: 39.24 | norm: 1.12\n",
      "step : 184 | loss: 5.436906814575195 | dt: 157.42 ms | tokens/sec: 39.03 | norm: 0.76\n",
      "step : 185 | loss: 5.356159210205078 | dt: 157.38 ms | tokens/sec: 39.04 | norm: 0.78\n",
      "step : 186 | loss: 5.130436897277832 | dt: 156.93 ms | tokens/sec: 39.15 | norm: 1.02\n",
      "step : 187 | loss: 5.30780029296875 | dt: 157.48 ms | tokens/sec: 39.02 | norm: 1.00\n",
      "step : 188 | loss: 5.493236541748047 | dt: 156.24 ms | tokens/sec: 39.32 | norm: 0.81\n",
      "step : 189 | loss: 5.378398895263672 | dt: 157.94 ms | tokens/sec: 38.90 | norm: 0.94\n",
      "step : 190 | loss: 5.3024139404296875 | dt: 159.41 ms | tokens/sec: 38.54 | norm: 1.11\n",
      "step : 191 | loss: 5.365626335144043 | dt: 157.24 ms | tokens/sec: 39.07 | norm: 0.97\n",
      "step : 192 | loss: 5.109683036804199 | dt: 157.38 ms | tokens/sec: 39.04 | norm: 0.84\n",
      "step : 193 | loss: 5.333756923675537 | dt: 157.63 ms | tokens/sec: 38.98 | norm: 1.05\n",
      "step : 194 | loss: 5.144561767578125 | dt: 157.65 ms | tokens/sec: 38.97 | norm: 1.24\n",
      "step : 195 | loss: 5.4297194480896 | dt: 157.42 ms | tokens/sec: 39.03 | norm: 0.89\n",
      "step : 196 | loss: 5.342639923095703 | dt: 156.88 ms | tokens/sec: 39.16 | norm: 0.90\n",
      "step : 197 | loss: 5.103095531463623 | dt: 157.27 ms | tokens/sec: 39.07 | norm: 1.08\n",
      "step : 198 | loss: 5.174883842468262 | dt: 157.12 ms | tokens/sec: 39.10 | norm: 0.90\n",
      "step : 199 | loss: 5.190428256988525 | dt: 158.19 ms | tokens/sec: 38.84 | norm: 0.76\n",
      "step : 200 | loss: 5.257665634155273 | dt: 159.79 ms | tokens/sec: 38.45 | norm: 0.82\n",
      "step : 201 | loss: 5.54468297958374 | dt: 158.38 ms | tokens/sec: 38.79 | norm: 1.24\n",
      "step : 202 | loss: 5.395493984222412 | dt: 155.88 ms | tokens/sec: 39.42 | norm: 1.21\n",
      "step : 203 | loss: 5.7064642906188965 | dt: 157.98 ms | tokens/sec: 38.89 | norm: 1.14\n",
      "step : 204 | loss: 5.523797512054443 | dt: 157.99 ms | tokens/sec: 38.89 | norm: 0.93\n",
      "step : 205 | loss: 5.371638298034668 | dt: 156.32 ms | tokens/sec: 39.30 | norm: 1.12\n",
      "step : 206 | loss: 5.391213893890381 | dt: 158.64 ms | tokens/sec: 38.73 | norm: 1.04\n",
      "step : 207 | loss: 5.23591423034668 | dt: 156.68 ms | tokens/sec: 39.21 | norm: 0.94\n",
      "step : 208 | loss: 5.019385814666748 | dt: 158.63 ms | tokens/sec: 38.73 | norm: 1.28\n",
      "step : 209 | loss: 5.191335678100586 | dt: 156.54 ms | tokens/sec: 39.25 | norm: 1.30\n",
      "step : 210 | loss: 5.319943904876709 | dt: 159.83 ms | tokens/sec: 38.44 | norm: 1.58\n",
      "step : 211 | loss: 5.1374125480651855 | dt: 159.25 ms | tokens/sec: 38.58 | norm: 1.18\n",
      "step : 212 | loss: 4.948364734649658 | dt: 157.33 ms | tokens/sec: 39.05 | norm: 1.00\n",
      "step : 213 | loss: 5.320703506469727 | dt: 157.80 ms | tokens/sec: 38.94 | norm: 1.00\n",
      "step : 214 | loss: 5.1006340980529785 | dt: 157.43 ms | tokens/sec: 39.03 | norm: 1.03\n",
      "step : 215 | loss: 5.014232635498047 | dt: 156.98 ms | tokens/sec: 39.14 | norm: 1.01\n",
      "step : 216 | loss: 5.133668422698975 | dt: 157.70 ms | tokens/sec: 38.96 | norm: 0.93\n",
      "step : 217 | loss: 4.8592424392700195 | dt: 156.73 ms | tokens/sec: 39.20 | norm: 1.02\n",
      "step : 218 | loss: 4.63920783996582 | dt: 157.85 ms | tokens/sec: 38.92 | norm: 1.09\n",
      "step : 219 | loss: 5.357047080993652 | dt: 157.17 ms | tokens/sec: 39.09 | norm: 1.12\n",
      "step : 220 | loss: 5.220297813415527 | dt: 161.05 ms | tokens/sec: 38.15 | norm: 1.19\n",
      "step : 221 | loss: 5.408573150634766 | dt: 156.72 ms | tokens/sec: 39.20 | norm: 1.17\n",
      "step : 222 | loss: 5.377988338470459 | dt: 158.01 ms | tokens/sec: 38.88 | norm: 1.28\n",
      "step : 223 | loss: 5.301436424255371 | dt: 158.05 ms | tokens/sec: 38.87 | norm: 1.25\n",
      "step : 224 | loss: 5.007259368896484 | dt: 156.52 ms | tokens/sec: 39.25 | norm: 1.32\n",
      "step : 225 | loss: 4.986018180847168 | dt: 158.52 ms | tokens/sec: 38.76 | norm: 0.97\n",
      "step : 226 | loss: 5.034757614135742 | dt: 156.60 ms | tokens/sec: 39.23 | norm: 1.17\n",
      "step : 227 | loss: 5.086313247680664 | dt: 158.45 ms | tokens/sec: 38.78 | norm: 1.02\n",
      "step : 228 | loss: 5.182342529296875 | dt: 156.45 ms | tokens/sec: 39.27 | norm: 1.04\n",
      "step : 229 | loss: 5.240437030792236 | dt: 157.49 ms | tokens/sec: 39.01 | norm: 1.49\n",
      "step : 230 | loss: 5.148383140563965 | dt: 160.80 ms | tokens/sec: 38.21 | norm: 1.27\n",
      "step : 231 | loss: 5.056256294250488 | dt: 157.36 ms | tokens/sec: 39.04 | norm: 1.11\n",
      "step : 232 | loss: 4.875588417053223 | dt: 157.79 ms | tokens/sec: 38.94 | norm: 1.03\n",
      "step : 233 | loss: 5.123192310333252 | dt: 157.14 ms | tokens/sec: 39.10 | norm: 0.99\n",
      "step : 234 | loss: 5.082334995269775 | dt: 157.07 ms | tokens/sec: 39.12 | norm: 1.05\n",
      "step : 235 | loss: 4.992484092712402 | dt: 159.17 ms | tokens/sec: 38.60 | norm: 1.03\n",
      "step : 236 | loss: 5.249060153961182 | dt: 157.39 ms | tokens/sec: 39.04 | norm: 0.79\n",
      "step : 237 | loss: 5.347434043884277 | dt: 157.85 ms | tokens/sec: 38.92 | norm: 0.86\n",
      "step : 238 | loss: 5.254220485687256 | dt: 156.64 ms | tokens/sec: 39.22 | norm: 0.85\n",
      "step : 239 | loss: 5.254216194152832 | dt: 159.99 ms | tokens/sec: 38.40 | norm: 0.76\n",
      "step : 240 | loss: 5.199912071228027 | dt: 158.86 ms | tokens/sec: 38.68 | norm: 0.91\n",
      "step : 241 | loss: 4.942126750946045 | dt: 159.26 ms | tokens/sec: 38.58 | norm: 1.07\n",
      "step : 242 | loss: 5.177979946136475 | dt: 157.54 ms | tokens/sec: 39.00 | norm: 0.83\n",
      "step : 243 | loss: 5.364991664886475 | dt: 158.82 ms | tokens/sec: 38.69 | norm: 0.81\n",
      "step : 244 | loss: 5.241996765136719 | dt: 156.41 ms | tokens/sec: 39.28 | norm: 0.92\n",
      "step : 245 | loss: 5.166469573974609 | dt: 156.27 ms | tokens/sec: 39.32 | norm: 0.93\n",
      "step : 246 | loss: 5.2523698806762695 | dt: 160.04 ms | tokens/sec: 38.39 | norm: 0.91\n",
      "step : 247 | loss: 4.974334716796875 | dt: 160.96 ms | tokens/sec: 38.17 | norm: 0.88\n",
      "step : 248 | loss: 5.210223197937012 | dt: 159.67 ms | tokens/sec: 38.48 | norm: 0.84\n",
      "step : 249 | loss: 5.000241756439209 | dt: 161.05 ms | tokens/sec: 38.15 | norm: 1.19\n",
      "step : 250 | loss: 5.284725666046143 | dt: 161.92 ms | tokens/sec: 37.94 | norm: 1.00\n",
      "step : 251 | loss: 5.181750297546387 | dt: 160.73 ms | tokens/sec: 38.23 | norm: 0.81\n",
      "step : 252 | loss: 4.928375244140625 | dt: 160.98 ms | tokens/sec: 38.17 | norm: 0.99\n",
      "step : 253 | loss: 5.013191223144531 | dt: 161.13 ms | tokens/sec: 38.13 | norm: 0.93\n",
      "step : 254 | loss: 5.052531719207764 | dt: 160.91 ms | tokens/sec: 38.18 | norm: 1.13\n",
      "step : 255 | loss: 5.110129356384277 | dt: 161.14 ms | tokens/sec: 38.13 | norm: 0.89\n",
      "step : 256 | loss: 5.403498649597168 | dt: 161.43 ms | tokens/sec: 38.06 | norm: 1.12\n",
      "step : 257 | loss: 5.269317626953125 | dt: 160.51 ms | tokens/sec: 38.28 | norm: 1.14\n",
      "step : 258 | loss: 5.57720947265625 | dt: 160.85 ms | tokens/sec: 38.20 | norm: 1.20\n",
      "step : 259 | loss: 5.38342809677124 | dt: 163.52 ms | tokens/sec: 37.57 | norm: 1.10\n",
      "step : 260 | loss: 5.225436210632324 | dt: 160.28 ms | tokens/sec: 38.33 | norm: 0.93\n",
      "step : 261 | loss: 5.259336948394775 | dt: 160.70 ms | tokens/sec: 38.23 | norm: 0.87\n",
      "step : 262 | loss: 5.110622882843018 | dt: 161.24 ms | tokens/sec: 38.10 | norm: 1.13\n",
      "step : 263 | loss: 4.948940753936768 | dt: 159.98 ms | tokens/sec: 38.41 | norm: 1.40\n",
      "step : 264 | loss: 5.0652947425842285 | dt: 160.60 ms | tokens/sec: 38.26 | norm: 1.08\n",
      "step : 265 | loss: 5.162578582763672 | dt: 160.86 ms | tokens/sec: 38.19 | norm: 1.28\n",
      "step : 266 | loss: 4.963984489440918 | dt: 160.32 ms | tokens/sec: 38.32 | norm: 1.27\n",
      "step : 267 | loss: 4.77156400680542 | dt: 161.30 ms | tokens/sec: 38.09 | norm: 0.95\n",
      "step : 268 | loss: 5.153325080871582 | dt: 160.49 ms | tokens/sec: 38.28 | norm: 0.95\n",
      "step : 269 | loss: 4.958991050720215 | dt: 162.05 ms | tokens/sec: 37.91 | norm: 1.18\n",
      "step : 270 | loss: 4.838151931762695 | dt: 161.19 ms | tokens/sec: 38.12 | norm: 1.16\n",
      "step : 271 | loss: 4.959855079650879 | dt: 160.01 ms | tokens/sec: 38.40 | norm: 0.84\n",
      "step : 272 | loss: 4.7101969718933105 | dt: 159.47 ms | tokens/sec: 38.53 | norm: 0.87\n",
      "step : 273 | loss: 4.4999589920043945 | dt: 160.58 ms | tokens/sec: 38.26 | norm: 1.22\n",
      "step : 274 | loss: 5.270779609680176 | dt: 159.52 ms | tokens/sec: 38.52 | norm: 1.23\n",
      "step : 275 | loss: 5.1106157302856445 | dt: 160.67 ms | tokens/sec: 38.24 | norm: 1.16\n",
      "step : 276 | loss: 5.251245498657227 | dt: 159.69 ms | tokens/sec: 38.47 | norm: 1.07\n",
      "step : 277 | loss: 5.246860027313232 | dt: 159.52 ms | tokens/sec: 38.52 | norm: 1.06\n",
      "step : 278 | loss: 5.196345329284668 | dt: 161.65 ms | tokens/sec: 38.01 | norm: 1.20\n",
      "step : 279 | loss: 4.862091064453125 | dt: 161.59 ms | tokens/sec: 38.02 | norm: 1.31\n",
      "step : 280 | loss: 4.817637920379639 | dt: 159.79 ms | tokens/sec: 38.45 | norm: 0.98\n",
      "step : 281 | loss: 4.879502773284912 | dt: 160.45 ms | tokens/sec: 38.29 | norm: 0.89\n",
      "step : 282 | loss: 4.9689040184021 | dt: 159.50 ms | tokens/sec: 38.52 | norm: 1.02\n",
      "step : 283 | loss: 5.0769267082214355 | dt: 160.41 ms | tokens/sec: 38.30 | norm: 1.23\n",
      "step : 284 | loss: 5.0859174728393555 | dt: 160.21 ms | tokens/sec: 38.35 | norm: 1.39\n",
      "step : 285 | loss: 5.03604793548584 | dt: 159.58 ms | tokens/sec: 38.50 | norm: 1.15\n",
      "step : 286 | loss: 4.929333686828613 | dt: 160.88 ms | tokens/sec: 38.19 | norm: 1.03\n",
      "step : 287 | loss: 4.744977951049805 | dt: 159.25 ms | tokens/sec: 38.58 | norm: 1.07\n",
      "step : 288 | loss: 4.994483470916748 | dt: 160.88 ms | tokens/sec: 38.19 | norm: 0.92\n",
      "step : 289 | loss: 4.902980327606201 | dt: 162.96 ms | tokens/sec: 37.70 | norm: 1.02\n",
      "step : 290 | loss: 4.815098762512207 | dt: 162.45 ms | tokens/sec: 37.82 | norm: 1.04\n",
      "step : 291 | loss: 5.107882022857666 | dt: 160.94 ms | tokens/sec: 38.18 | norm: 0.85\n",
      "step : 292 | loss: 5.205259323120117 | dt: 161.64 ms | tokens/sec: 38.01 | norm: 0.83\n",
      "step : 293 | loss: 5.108821392059326 | dt: 162.79 ms | tokens/sec: 37.74 | norm: 0.92\n",
      "step : 294 | loss: 5.145040512084961 | dt: 162.19 ms | tokens/sec: 37.88 | norm: 0.89\n",
      "step : 295 | loss: 5.0633368492126465 | dt: 161.07 ms | tokens/sec: 38.14 | norm: 0.88\n",
      "step : 296 | loss: 4.767645835876465 | dt: 162.85 ms | tokens/sec: 37.73 | norm: 0.89\n",
      "step : 297 | loss: 5.038567543029785 | dt: 162.48 ms | tokens/sec: 37.81 | norm: 1.00\n",
      "step : 298 | loss: 5.2433085441589355 | dt: 164.12 ms | tokens/sec: 37.44 | norm: 1.15\n",
      "step : 299 | loss: 5.096162796020508 | dt: 161.46 ms | tokens/sec: 38.05 | norm: 1.07\n",
      "step : 300 | loss: 5.0054450035095215 | dt: 162.12 ms | tokens/sec: 37.90 | norm: 0.95\n",
      "step : 301 | loss: 5.104757308959961 | dt: 163.90 ms | tokens/sec: 37.49 | norm: 0.97\n",
      "step : 302 | loss: 4.824770927429199 | dt: 167.06 ms | tokens/sec: 36.78 | norm: 1.05\n",
      "step : 303 | loss: 5.044689655303955 | dt: 159.67 ms | tokens/sec: 38.48 | norm: 0.76\n",
      "step : 304 | loss: 4.860566139221191 | dt: 160.45 ms | tokens/sec: 38.29 | norm: 0.99\n",
      "step : 305 | loss: 5.158478736877441 | dt: 162.45 ms | tokens/sec: 37.82 | norm: 0.80\n",
      "step : 306 | loss: 5.078934669494629 | dt: 161.32 ms | tokens/sec: 38.09 | norm: 0.79\n",
      "step : 307 | loss: 4.821175575256348 | dt: 160.16 ms | tokens/sec: 38.36 | norm: 0.92\n",
      "step : 308 | loss: 4.887901306152344 | dt: 162.51 ms | tokens/sec: 37.81 | norm: 0.83\n",
      "step : 309 | loss: 4.915997505187988 | dt: 158.12 ms | tokens/sec: 38.86 | norm: 0.95\n",
      "step : 310 | loss: 4.97357702255249 | dt: 159.55 ms | tokens/sec: 38.51 | norm: 0.89\n",
      "step : 311 | loss: 5.291261672973633 | dt: 157.56 ms | tokens/sec: 39.00 | norm: 1.18\n",
      "step : 312 | loss: 5.160312652587891 | dt: 160.32 ms | tokens/sec: 38.32 | norm: 0.99\n",
      "step : 313 | loss: 5.479999542236328 | dt: 156.95 ms | tokens/sec: 39.15 | norm: 0.96\n",
      "step : 314 | loss: 5.287103652954102 | dt: 159.55 ms | tokens/sec: 38.51 | norm: 0.98\n",
      "step : 315 | loss: 5.109981536865234 | dt: 156.96 ms | tokens/sec: 39.14 | norm: 0.96\n",
      "step : 316 | loss: 5.148643493652344 | dt: 159.13 ms | tokens/sec: 38.61 | norm: 0.94\n",
      "step : 317 | loss: 4.9826483726501465 | dt: 157.57 ms | tokens/sec: 38.99 | norm: 0.90\n",
      "step : 318 | loss: 4.824186325073242 | dt: 160.94 ms | tokens/sec: 38.18 | norm: 1.24\n",
      "step : 319 | loss: 4.9434814453125 | dt: 159.62 ms | tokens/sec: 38.49 | norm: 1.07\n",
      "step : 320 | loss: 5.027608871459961 | dt: 157.59 ms | tokens/sec: 38.99 | norm: 1.15\n",
      "step : 321 | loss: 4.840023994445801 | dt: 161.21 ms | tokens/sec: 38.11 | norm: 1.09\n",
      "step : 322 | loss: 4.612892150878906 | dt: 156.81 ms | tokens/sec: 39.18 | norm: 0.89\n",
      "step : 323 | loss: 5.023815155029297 | dt: 158.62 ms | tokens/sec: 38.73 | norm: 0.94\n",
      "step : 324 | loss: 4.880466461181641 | dt: 158.07 ms | tokens/sec: 38.87 | norm: 1.11\n",
      "step : 325 | loss: 4.7374162673950195 | dt: 157.19 ms | tokens/sec: 39.09 | norm: 1.10\n",
      "step : 326 | loss: 4.851127624511719 | dt: 157.99 ms | tokens/sec: 38.89 | norm: 0.90\n",
      "step : 327 | loss: 4.589635372161865 | dt: 158.88 ms | tokens/sec: 38.67 | norm: 0.80\n",
      "step : 328 | loss: 4.416269779205322 | dt: 158.35 ms | tokens/sec: 38.80 | norm: 1.08\n",
      "step : 329 | loss: 5.142974376678467 | dt: 158.01 ms | tokens/sec: 38.88 | norm: 1.14\n",
      "step : 330 | loss: 4.977571487426758 | dt: 157.90 ms | tokens/sec: 38.91 | norm: 1.21\n",
      "step : 331 | loss: 5.1371307373046875 | dt: 158.65 ms | tokens/sec: 38.73 | norm: 1.20\n",
      "step : 332 | loss: 5.143978118896484 | dt: 157.50 ms | tokens/sec: 39.01 | norm: 1.17\n",
      "step : 333 | loss: 5.078032970428467 | dt: 161.21 ms | tokens/sec: 38.11 | norm: 1.12\n",
      "step : 334 | loss: 4.772658824920654 | dt: 157.07 ms | tokens/sec: 39.12 | norm: 1.32\n",
      "step : 335 | loss: 4.727270126342773 | dt: 158.21 ms | tokens/sec: 38.84 | norm: 1.08\n",
      "step : 336 | loss: 4.739230632781982 | dt: 158.44 ms | tokens/sec: 38.78 | norm: 0.94\n",
      "step : 337 | loss: 4.823707580566406 | dt: 161.07 ms | tokens/sec: 38.15 | norm: 0.82\n",
      "step : 338 | loss: 4.925826072692871 | dt: 160.96 ms | tokens/sec: 38.17 | norm: 0.79\n",
      "step : 339 | loss: 4.8834075927734375 | dt: 157.05 ms | tokens/sec: 39.12 | norm: 1.23\n",
      "step : 340 | loss: 4.85447883605957 | dt: 158.27 ms | tokens/sec: 38.82 | norm: 1.15\n",
      "step : 341 | loss: 4.772372245788574 | dt: 158.29 ms | tokens/sec: 38.81 | norm: 0.97\n",
      "step : 342 | loss: 4.577090263366699 | dt: 157.32 ms | tokens/sec: 39.05 | norm: 0.99\n",
      "step : 343 | loss: 4.8607177734375 | dt: 159.06 ms | tokens/sec: 38.63 | norm: 0.85\n",
      "step : 344 | loss: 4.783627033233643 | dt: 156.62 ms | tokens/sec: 39.23 | norm: 1.02\n",
      "step : 345 | loss: 4.704024314880371 | dt: 160.13 ms | tokens/sec: 38.37 | norm: 1.05\n",
      "step : 346 | loss: 5.009582996368408 | dt: 157.30 ms | tokens/sec: 39.06 | norm: 0.85\n",
      "step : 347 | loss: 5.11308479309082 | dt: 159.34 ms | tokens/sec: 38.56 | norm: 0.89\n",
      "step : 348 | loss: 4.995410919189453 | dt: 159.19 ms | tokens/sec: 38.59 | norm: 0.75\n",
      "step : 349 | loss: 4.9968767166137695 | dt: 157.04 ms | tokens/sec: 39.12 | norm: 0.74\n",
      "step : 350 | loss: 4.924272537231445 | dt: 160.28 ms | tokens/sec: 38.33 | norm: 0.81\n",
      "step : 351 | loss: 4.650615692138672 | dt: 157.21 ms | tokens/sec: 39.08 | norm: 1.06\n",
      "step : 352 | loss: 4.932771682739258 | dt: 158.68 ms | tokens/sec: 38.72 | norm: 0.95\n",
      "step : 353 | loss: 5.135815143585205 | dt: 157.62 ms | tokens/sec: 38.98 | norm: 0.83\n",
      "step : 354 | loss: 5.033229827880859 | dt: 157.32 ms | tokens/sec: 39.06 | norm: 1.12\n",
      "step : 355 | loss: 4.894093990325928 | dt: 159.39 ms | tokens/sec: 38.55 | norm: 1.13\n",
      "step : 356 | loss: 5.043175220489502 | dt: 156.88 ms | tokens/sec: 39.16 | norm: 1.14\n",
      "step : 357 | loss: 4.774924278259277 | dt: 161.81 ms | tokens/sec: 37.97 | norm: 0.97\n",
      "step : 358 | loss: 5.008806228637695 | dt: 159.44 ms | tokens/sec: 38.54 | norm: 0.88\n",
      "step : 359 | loss: 4.783020973205566 | dt: 156.96 ms | tokens/sec: 39.14 | norm: 1.19\n",
      "step : 360 | loss: 5.077484130859375 | dt: 159.49 ms | tokens/sec: 38.52 | norm: 0.99\n",
      "step : 361 | loss: 4.984368801116943 | dt: 157.60 ms | tokens/sec: 38.99 | norm: 0.89\n",
      "step : 362 | loss: 4.713126182556152 | dt: 160.26 ms | tokens/sec: 38.34 | norm: 0.98\n",
      "step : 363 | loss: 4.776266098022461 | dt: 157.00 ms | tokens/sec: 39.13 | norm: 0.94\n",
      "step : 364 | loss: 4.823215484619141 | dt: 157.92 ms | tokens/sec: 38.91 | norm: 0.97\n",
      "step : 365 | loss: 4.884620666503906 | dt: 158.25 ms | tokens/sec: 38.82 | norm: 0.98\n",
      "step : 366 | loss: 5.183565139770508 | dt: 158.34 ms | tokens/sec: 38.80 | norm: 1.21\n",
      "step : 367 | loss: 5.056493759155273 | dt: 161.34 ms | tokens/sec: 38.08 | norm: 1.10\n",
      "step : 368 | loss: 5.375974178314209 | dt: 158.78 ms | tokens/sec: 38.69 | norm: 1.03\n",
      "step : 369 | loss: 5.15786075592041 | dt: 158.83 ms | tokens/sec: 38.68 | norm: 0.93\n",
      "step : 370 | loss: 4.997767448425293 | dt: 158.99 ms | tokens/sec: 38.64 | norm: 0.94\n",
      "step : 371 | loss: 5.05582857131958 | dt: 156.94 ms | tokens/sec: 39.15 | norm: 0.97\n",
      "step : 372 | loss: 4.91095495223999 | dt: 159.83 ms | tokens/sec: 38.44 | norm: 1.02\n",
      "step : 373 | loss: 4.713303565979004 | dt: 157.18 ms | tokens/sec: 39.09 | norm: 1.13\n",
      "step : 374 | loss: 4.879264831542969 | dt: 159.39 ms | tokens/sec: 38.55 | norm: 1.19\n",
      "step : 375 | loss: 4.940699577331543 | dt: 157.08 ms | tokens/sec: 39.11 | norm: 1.24\n",
      "step : 376 | loss: 4.725263595581055 | dt: 157.87 ms | tokens/sec: 38.92 | norm: 1.09\n",
      "step : 377 | loss: 4.519977569580078 | dt: 157.93 ms | tokens/sec: 38.90 | norm: 0.94\n",
      "step : 378 | loss: 4.934945583343506 | dt: 157.80 ms | tokens/sec: 38.93 | norm: 0.99\n",
      "step : 379 | loss: 4.783537864685059 | dt: 160.09 ms | tokens/sec: 38.38 | norm: 1.01\n",
      "step : 380 | loss: 4.649980068206787 | dt: 159.48 ms | tokens/sec: 38.53 | norm: 1.12\n",
      "step : 381 | loss: 4.793776512145996 | dt: 157.15 ms | tokens/sec: 39.10 | norm: 0.98\n",
      "step : 382 | loss: 4.545551300048828 | dt: 158.85 ms | tokens/sec: 38.68 | norm: 0.89\n",
      "step : 383 | loss: 4.296041011810303 | dt: 157.55 ms | tokens/sec: 39.00 | norm: 0.97\n",
      "step : 384 | loss: 5.061831951141357 | dt: 159.74 ms | tokens/sec: 38.46 | norm: 1.15\n",
      "step : 385 | loss: 4.907710075378418 | dt: 158.42 ms | tokens/sec: 38.78 | norm: 1.26\n",
      "step : 386 | loss: 5.038883209228516 | dt: 158.05 ms | tokens/sec: 38.87 | norm: 1.12\n",
      "step : 387 | loss: 5.042031764984131 | dt: 162.95 ms | tokens/sec: 37.70 | norm: 1.18\n",
      "step : 388 | loss: 4.982985973358154 | dt: 158.31 ms | tokens/sec: 38.81 | norm: 1.17\n",
      "step : 389 | loss: 4.624807357788086 | dt: 160.11 ms | tokens/sec: 38.37 | norm: 1.19\n",
      "step : 390 | loss: 4.585797309875488 | dt: 157.85 ms | tokens/sec: 38.92 | norm: 0.82\n",
      "step : 391 | loss: 4.653561592102051 | dt: 159.41 ms | tokens/sec: 38.54 | norm: 0.94\n",
      "step : 392 | loss: 4.75707483291626 | dt: 158.02 ms | tokens/sec: 38.88 | norm: 0.81\n",
      "step : 393 | loss: 4.851609230041504 | dt: 158.07 ms | tokens/sec: 38.87 | norm: 0.83\n",
      "step : 394 | loss: 4.854250907897949 | dt: 161.14 ms | tokens/sec: 38.13 | norm: 1.33\n",
      "step : 395 | loss: 4.773107528686523 | dt: 157.20 ms | tokens/sec: 39.09 | norm: 0.97\n",
      "step : 396 | loss: 4.687314987182617 | dt: 159.25 ms | tokens/sec: 38.58 | norm: 0.87\n",
      "step : 397 | loss: 4.523886680603027 | dt: 157.01 ms | tokens/sec: 39.13 | norm: 0.98\n",
      "step : 398 | loss: 4.762599468231201 | dt: 159.08 ms | tokens/sec: 38.62 | norm: 0.95\n",
      "step : 399 | loss: 4.663570880889893 | dt: 158.06 ms | tokens/sec: 38.87 | norm: 0.98\n",
      "step : 400 | loss: 4.599340438842773 | dt: 160.26 ms | tokens/sec: 38.34 | norm: 1.03\n",
      "step : 401 | loss: 4.909390926361084 | dt: 159.53 ms | tokens/sec: 38.51 | norm: 0.78\n",
      "step : 402 | loss: 5.054086685180664 | dt: 159.08 ms | tokens/sec: 38.62 | norm: 0.91\n",
      "step : 403 | loss: 4.962043285369873 | dt: 159.41 ms | tokens/sec: 38.54 | norm: 0.94\n",
      "step : 404 | loss: 4.963242530822754 | dt: 158.89 ms | tokens/sec: 38.67 | norm: 0.88\n",
      "step : 405 | loss: 4.890934944152832 | dt: 157.02 ms | tokens/sec: 39.13 | norm: 0.75\n",
      "step : 406 | loss: 4.572844505310059 | dt: 159.21 ms | tokens/sec: 38.59 | norm: 0.90\n",
      "step : 407 | loss: 4.853170394897461 | dt: 161.28 ms | tokens/sec: 38.10 | norm: 0.91\n",
      "step : 408 | loss: 5.0720720291137695 | dt: 160.06 ms | tokens/sec: 38.38 | norm: 0.86\n",
      "step : 409 | loss: 4.958823204040527 | dt: 158.98 ms | tokens/sec: 38.65 | norm: 0.90\n",
      "step : 410 | loss: 4.841050148010254 | dt: 157.27 ms | tokens/sec: 39.07 | norm: 0.95\n",
      "step : 411 | loss: 4.943399906158447 | dt: 159.92 ms | tokens/sec: 38.42 | norm: 0.87\n",
      "step : 412 | loss: 4.6903910636901855 | dt: 157.97 ms | tokens/sec: 38.89 | norm: 0.90\n",
      "step : 413 | loss: 4.886216640472412 | dt: 160.84 ms | tokens/sec: 38.20 | norm: 0.81\n",
      "step : 414 | loss: 4.677769660949707 | dt: 157.58 ms | tokens/sec: 38.99 | norm: 1.13\n",
      "step : 415 | loss: 5.0313615798950195 | dt: 159.17 ms | tokens/sec: 38.60 | norm: 1.05\n",
      "step : 416 | loss: 4.924491882324219 | dt: 158.58 ms | tokens/sec: 38.74 | norm: 1.03\n",
      "step : 417 | loss: 4.633150100708008 | dt: 157.78 ms | tokens/sec: 38.94 | norm: 1.07\n",
      "step : 418 | loss: 4.722278118133545 | dt: 158.94 ms | tokens/sec: 38.66 | norm: 0.93\n",
      "step : 419 | loss: 4.740322589874268 | dt: 158.13 ms | tokens/sec: 38.86 | norm: 0.81\n",
      "step : 420 | loss: 4.787724494934082 | dt: 160.91 ms | tokens/sec: 38.18 | norm: 0.99\n",
      "step : 421 | loss: 5.111544609069824 | dt: 158.31 ms | tokens/sec: 38.81 | norm: 1.31\n",
      "step : 422 | loss: 5.0014495849609375 | dt: 157.96 ms | tokens/sec: 38.90 | norm: 1.29\n",
      "step : 423 | loss: 5.346441268920898 | dt: 166.62 ms | tokens/sec: 36.87 | norm: 1.34\n",
      "step : 424 | loss: 5.136754989624023 | dt: 173.22 ms | tokens/sec: 35.47 | norm: 1.00\n",
      "step : 425 | loss: 4.978687286376953 | dt: 162.84 ms | tokens/sec: 37.73 | norm: 0.89\n",
      "step : 426 | loss: 5.033073425292969 | dt: 160.59 ms | tokens/sec: 38.26 | norm: 1.02\n",
      "step : 427 | loss: 4.8678388595581055 | dt: 158.69 ms | tokens/sec: 38.72 | norm: 1.13\n",
      "step : 428 | loss: 4.70631742477417 | dt: 159.08 ms | tokens/sec: 38.62 | norm: 1.33\n",
      "step : 429 | loss: 4.82564115524292 | dt: 159.05 ms | tokens/sec: 38.63 | norm: 1.17\n",
      "step : 430 | loss: 4.8578314781188965 | dt: 158.61 ms | tokens/sec: 38.74 | norm: 0.98\n",
      "step : 431 | loss: 4.648952007293701 | dt: 159.88 ms | tokens/sec: 38.43 | norm: 0.99\n",
      "step : 432 | loss: 4.447319984436035 | dt: 158.73 ms | tokens/sec: 38.71 | norm: 1.00\n",
      "step : 433 | loss: 4.8841657638549805 | dt: 159.84 ms | tokens/sec: 38.44 | norm: 1.18\n",
      "step : 434 | loss: 4.732288360595703 | dt: 158.94 ms | tokens/sec: 38.66 | norm: 1.07\n",
      "step : 435 | loss: 4.581002235412598 | dt: 161.06 ms | tokens/sec: 38.15 | norm: 1.00\n",
      "step : 436 | loss: 4.700783729553223 | dt: 158.60 ms | tokens/sec: 38.74 | norm: 0.89\n",
      "step : 437 | loss: 4.451943397521973 | dt: 159.55 ms | tokens/sec: 38.51 | norm: 0.87\n",
      "step : 438 | loss: 4.188806533813477 | dt: 158.39 ms | tokens/sec: 38.79 | norm: 1.03\n",
      "step : 439 | loss: 4.948601245880127 | dt: 158.05 ms | tokens/sec: 38.87 | norm: 0.97\n",
      "step : 440 | loss: 4.833083152770996 | dt: 157.71 ms | tokens/sec: 38.96 | norm: 1.10\n",
      "step : 441 | loss: 4.932592391967773 | dt: 159.22 ms | tokens/sec: 38.59 | norm: 0.98\n",
      "step : 442 | loss: 4.933711528778076 | dt: 156.92 ms | tokens/sec: 39.15 | norm: 1.08\n",
      "step : 443 | loss: 4.8731207847595215 | dt: 160.94 ms | tokens/sec: 38.18 | norm: 1.09\n",
      "step : 444 | loss: 4.539542198181152 | dt: 158.36 ms | tokens/sec: 38.80 | norm: 1.18\n",
      "step : 445 | loss: 4.496020317077637 | dt: 159.30 ms | tokens/sec: 38.57 | norm: 0.81\n",
      "step : 446 | loss: 4.565985679626465 | dt: 158.66 ms | tokens/sec: 38.72 | norm: 0.92\n",
      "step : 447 | loss: 4.669699668884277 | dt: 159.09 ms | tokens/sec: 38.62 | norm: 0.85\n",
      "step : 448 | loss: 4.759407043457031 | dt: 158.35 ms | tokens/sec: 38.80 | norm: 0.85\n",
      "step : 449 | loss: 4.767548561096191 | dt: 158.63 ms | tokens/sec: 38.73 | norm: 1.27\n",
      "step : 450 | loss: 4.691272735595703 | dt: 158.43 ms | tokens/sec: 38.78 | norm: 1.00\n",
      "step : 451 | loss: 4.61977481842041 | dt: 159.22 ms | tokens/sec: 38.59 | norm: 0.99\n",
      "step : 452 | loss: 4.464212417602539 | dt: 157.51 ms | tokens/sec: 39.01 | norm: 0.99\n",
      "step : 453 | loss: 4.711751461029053 | dt: 159.76 ms | tokens/sec: 38.46 | norm: 0.95\n",
      "step : 454 | loss: 4.571901321411133 | dt: 158.06 ms | tokens/sec: 38.87 | norm: 1.05\n",
      "step : 455 | loss: 4.494922637939453 | dt: 159.19 ms | tokens/sec: 38.59 | norm: 1.09\n",
      "step : 456 | loss: 4.823160171508789 | dt: 158.21 ms | tokens/sec: 38.83 | norm: 0.99\n",
      "step : 457 | loss: 4.93138313293457 | dt: 157.67 ms | tokens/sec: 38.97 | norm: 0.84\n",
      "step : 458 | loss: 4.843502998352051 | dt: 158.99 ms | tokens/sec: 38.64 | norm: 0.89\n"
     ]
    }
   ],
   "source": [
    "start= time.time()\n",
    "for i in range(max_steps):\n",
    "    t0 = time.time()\n",
    "    x, y = train_loader.next_batch()\n",
    "    x, y = x.to(device=device), y.to(device)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "        logits, loss = model(x, y)\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    # grad clip\n",
    "    norm = torch.nn.utils.clip_grad_norm(model.parameters(), 1.0)\n",
    "\n",
    "    lr = get_lr(i)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr \n",
    "\n",
    "    optimizer.step()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    t1 = time.time() \n",
    "    dt = (t1 - t0) * 1000\n",
    "    tokens_per_sec = (train_loader.B * train_loader.T) / (dt)\n",
    "    \n",
    "    # print happens via CPU, hence wait (synchronize GPU)\n",
    "    print(f'step : {i+1} | loss: {loss.item()} | dt: {dt:.2f} ms | tokens/sec: {tokens_per_sec:.2f} | norm: {norm:.2f}')\n",
    "    \n",
    "\n",
    "\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final loss:  tensor(0.6168, device='cuda:0', grad_fn=<CompiledFunctionBackward>)\n",
      "total time: 842.1233501434326 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"final loss: \", loss)\n",
    "print(f\"total time: {end - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"5k-run.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
